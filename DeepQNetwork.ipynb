{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modified from https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    ''' buffer to hold and sample (state, action, reward, next state) tuples'''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        '''add tuple for state transition'''\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        '''sample uniformly from buffer'''\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''return current length of buffer'''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    '''compute TD(0) loss for batch_size number of samples'''\n",
    "    # sample uniformly from buffer of previous state action tuples\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    # convert to torch\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    with torch.no_grad():\n",
    "        next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    # forward pass for Q-value for state and next_state\n",
    "    q_values      = model.forward(state)\n",
    "    next_q_values = model.forward(next_state)\n",
    "    \n",
    "    # get Q- value of action taken\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    # max over next actions\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    # discounted expected future rewards\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    # squared TD error\n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    '''plot intermediate results'''\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class one_hot_hand_target:\n",
    "    '''agent class. Hand and cursor are one hot pixels in separate inputs channels'''\n",
    "    def __init__(self,npixelx,npixely,king_moves=False):\n",
    "        self.hand_set = False # bool whether hand is placed\n",
    "        self.target_set = False # bool whether target is placed\n",
    "        self.npixelx = npixelx # # size of screen in x\n",
    "        self.npixely = npixely # size of screen in y \n",
    "        self.observation_shape = [2,npixelx,npixely] # shape of inputs\n",
    "        self.target_radius = 1 # pixel radius considered as reaching the target \n",
    "        \n",
    "        \n",
    "        # encode hand state as one hot matrix - leave empty \n",
    "        self.hand = None \n",
    "        \n",
    "        # encode target state - leave empty until target is placed\n",
    "        self.target = None \n",
    "        \n",
    "        \n",
    "        if king_moves: # allow diagonal moves\n",
    "            self.num_actions = 8\n",
    "            self.actions = np.array([[1,0],\n",
    "                           [0,1],\n",
    "                           [-1,0],\n",
    "                           [0,-1],\n",
    "                           [1,1],\n",
    "                           [1,-1],\n",
    "                           [-1,1],\n",
    "                           [-1,-1]])\n",
    "        else:\n",
    "            self.num_actions = 4\n",
    "            self.actions = np.array([[1,0],\n",
    "                           [0,1],\n",
    "                           [-1,0],\n",
    "                           [0,-1]])\n",
    "            \n",
    "    def place_hand(self,x,y):\n",
    "        self.hand = np.array([x,y])\n",
    "        self.hand_set=True\n",
    "        \n",
    "        \n",
    "    def place_target(self,x,y):\n",
    "        self.target = np.array([x,y])\n",
    "        self.target_set = True\n",
    "        \n",
    "        \n",
    "    def init_trial(self,hx,hy,tx,ty):\n",
    "        # place hand\n",
    "        self.place_hand(hx,hy)\n",
    "        \n",
    "        # place target\n",
    "        self.place_target(tx,ty)\n",
    "        \n",
    "        \n",
    "        # calculate reward\n",
    "        reward = self.get_reward()\n",
    "        if reward>-1:\n",
    "          return self.get_state(),0,True\n",
    "        else:\n",
    "          return self.get_state(),-1,False\n",
    "        \n",
    "    def get_reward(self):\n",
    "        if np.linalg.norm(self.target-self.hand,2)<=self.target_radius:\n",
    "          return 0\n",
    "        else:\n",
    "          return -1\n",
    "        \n",
    "\n",
    "    def get_state(self):\n",
    "        screen = np.zeros((2,self.npixelx,self.npixely))\n",
    "        screen[0,self.hand[0],self.hand[1]]+=10\n",
    "        screen[1,self.target[0],self.target[1]]+=10\n",
    "        \n",
    "        return screen \n",
    "    \n",
    "    \n",
    "    def step(self,a):\n",
    "\n",
    "        action = self.actions[a,:]\n",
    "        self.hand[0]= np.minimum(np.maximum(self.hand[0]+action[0],0),self.npixelx-1)\n",
    "        self.hand[1]= np.minimum(np.maximum(self.hand[1]+action[1],0),self.npixely-1)\n",
    "        \n",
    "        \n",
    "        reward = self.get_reward()\n",
    "        if reward>-1:\n",
    "          return self.get_state(),0,True\n",
    "        else:\n",
    "          return self.get_state(),-1,False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    ''' Deep Q-Network from pixels '''\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "\n",
    "#         # 64 x 64 settings - doesn't quite work yet, exploding gradients, loss divergerges at later stages of training\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(input_shape[0], 2, kernel_size=5, stride=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(2, 4, kernel_size=5, stride=1),\n",
    "#             nn.ReLU(),\n",
    "#             # nn.Conv2d(8, 32, kernel_size=1, stride=1),\n",
    "#             # nn.ReLU()\n",
    "#         )\n",
    "# #         print(self.features)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(self.feature_size(), 64),\n",
    "#             nn.ReLU(),\n",
    "#             # nn.Dropout(p=0),\n",
    "#             nn.Linear(64,64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, self.num_actions),\n",
    "#         )\n",
    "\n",
    "        # 16 x 16 solution\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 8, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=.3), # this layer doesn't seem terribly important\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.num_actions)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        '''epsilon greedy behavior'''\n",
    "        if random.random() > epsilon: # get action with highest Q-value\n",
    "            with torch.no_grad():\n",
    "                state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0))\n",
    "            q_value = self.forward(state)\n",
    "            action  = torch.argmax(torch.squeeze(q_value)).item() \n",
    "        else: # behave randomly\n",
    "            action = random.randrange(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bcbe499388>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters for controlling epsilon-greedy behavior\n",
    "epsilon_start = 1\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 30000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "plt.plot([epsilon_by_frame(i) for i in range(100000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 16, 16]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d8f195b254b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtarget_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m           \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtarget_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mall_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9d0279a14196>\u001b[0m in \u001b[0;36minit_trial\u001b[1;34m(self, hx, hy, tx, ty)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9d0279a14196>\u001b[0m in \u001b[0;36mget_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mscreen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnpixelx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnpixely\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mscreen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhand\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhand\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mscreen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "replay_initial = 1000 # frame for first replay batch\n",
    "replay_buffer = ReplayBuffer(100000) # initialize replay buffer\n",
    "\n",
    "num_frames = 100000 # number of total frames \n",
    "batch_size = 32 # number of samples to take in replay cycle\n",
    "gamma      = 1. # discounting factor\n",
    "npix = 16  # size of screen\n",
    "\n",
    "losses = [] # initialize losses\n",
    "all_rewards = [] # initialize rewards per episode\n",
    "episode_reward = 0 # initialize reward counter\n",
    "max_episode_length=100000 # when to quit an episode\n",
    "episode_length = 0 # counter to check against max_episode_length\n",
    "episode = 0 # episode counter\n",
    "epsilon_counter = 0 # index for tracking epsilon (randomness of behavior)\n",
    "\n",
    "h = one_hot_hand_target(npix,npix,king_moves=True) # initialize agent\n",
    "h.target_radius = 8 # start with wide radius to speed up learning\n",
    "model = CnnDQN(h.observation_shape, h.num_actions) # initialize model\n",
    "\n",
    "# push model to GPU\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "# init optimizer\n",
    "optimizer = optim.Adam(model.parameters(),lr=1E-3)\n",
    "\n",
    "# place target and initialize trial\n",
    "target_loc = [np.random.randint(npix),np.random.randint(npix)]\n",
    "state,_,reward = h.init_trial(np.random.randint(npix),np.random.randint(npix),*target_loc)\n",
    "\n",
    "for frame_idx in range(1,num_frames + 1):\n",
    "    epsilon = epsilon_by_episode(epsilon_counter) # set randomnesss of actions\n",
    "    action = model.act(state, epsilon) \n",
    "    next_state, reward, done = h.step(action)\n",
    "    if reward==0: # print when trial is successfully finished, for debugging mostly\n",
    "        print(reward)\n",
    "    replay_buffer.push(state, action, reward, next_state, done) # add to buffer\n",
    "    \n",
    "    # update state and counters\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    episode_length+=1\n",
    "    epsilon_counter+=1\n",
    "    \n",
    "    # if episode is complete, or the trial is stuck\n",
    "    if done or (episode_length>max_episode_length):\n",
    "        episode+=1 \n",
    "        episode_length=0\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        # if successfully performed 1000 episodes, decrease target radius\n",
    "        if episode%1000==0:\n",
    "            h.target_radius = np.maximum(h.target_radius/2,1)\n",
    "            epsilon_counter=0\n",
    "            \n",
    "            \n",
    "        # initialize new trial\n",
    "        target_loc = [np.random.randint(npix),np.random.randint(npix)]\n",
    "        # ensure hand is not within target radius of target to start\n",
    "        done=True\n",
    "        while done:\n",
    "          state , _, done = h.init_trial(np.random.randint(npix),np.random.randint(npix),*target_loc)\n",
    "        \n",
    "    # calculate TD(0) loss and perform backprop\n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size) \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    # plot intermediate results\n",
    "    if frame_idx % 1000 == 0:\n",
    "        plot(frame_idx, all_rewards[-10000:], losses[-10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"DQN_16x16.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
